(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[327],{9182:function(e,l,s){(window.__NEXT_P=window.__NEXT_P||[]).push(["/projects",function(){return s(1256)}])},1256:function(e,l,s){"use strict";s.r(l),s.d(l,{default:function(){return r}});var t=s(5893);function r(){return(0,t.jsx)("div",{className:"w-full flex justify-center py-5 pt-16 h-full",children:(0,t.jsxs)("div",{className:"container px-5 h-full flex-col flex",children:[(0,t.jsx)("h1",{className:"text-8xl md:text-9xl font-bold pb-2",children:"PROJECTS"}),(0,t.jsx)("div",{className:"flex md:items-center h-full md:mb-20",children:(0,t.jsxs)("div",{className:"flex flex-col md:flex-row md:justify-center child:md:w-2/5 child:border child:border-white child:bg-sky child:p-3 hover:child:bg-white hover:child:text-sky child:transition-colors child:cursor-pointer child:flex child:flex-col",children:[(0,t.jsxs)("div",{className:"mb-5 md:mb-0 md:mr-5",children:[(0,t.jsx)("p",{className:"text-4xl pb-1",children:"LAION-400M"}),(0,t.jsx)("hr",{}),(0,t.jsxs)("p",{className:"pt-2 text-lg",children:["We are creating the world\u2019s largest openly accessible image-text-pair dataset, as a step to democratize research on training of large language-image models like DALL-E & CLIP",(0,t.jsx)("br",{})," ",(0,t.jsx)("br",{}),"Explore our already released samples (400M) & help us to quickly grow into the billions!"]})]}),(0,t.jsxs)("div",{className:"",children:[(0,t.jsx)("p",{className:"text-4xl pb-1",children:"LAION-5B"}),(0,t.jsx)("hr",{}),(0,t.jsx)("p",{className:"pt-2 text-lg",children:"To democratize research on large-scale multi-modal models, we present LAION-5B \u2013 a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, featuring several nearest neighbor indices, an improved web-interface for exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection."})]})]})})]})})}}},function(e){e.O(0,[774,888,179],(function(){return l=9182,e(e.s=l);var l}));var l=e.O();_N_E=l}]);